{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import join\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddings():\n",
    "    \"\"\"\n",
    "    provides uniform access to word vectors (fastText, GloVe, ConceptNet Numberbatch)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_type, base_dir='./word_vectors'):\n",
    "        self.embedding_type = embedding_type\n",
    "        self.word_vectors, self.vector_labels = read_embeddings(embedding_type, base_dir)\n",
    "        self.dict = make_embedding_map(self.word_vectors, self.vector_labels)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        dict-like access to word vectors\n",
    "        \"\"\"\n",
    "        if self.embedding_type.lower() in ['word2vec', 'glove', 'conceptnet', 'conceptnetnb']:\n",
    "            # lowercase key: no capitals in word2vec, GloVe and ConceptNet Numberbatch\n",
    "            key = key.lower()\n",
    "        return self.dict.get(key, None)\n",
    "    \n",
    "    def get_cosine_similarity(self, w1, w2):\n",
    "\n",
    "        v1 = self[w1]\n",
    "        if v1 is not None:\n",
    "            v1 = v1.reshape(1, -1)\n",
    "        else:\n",
    "            print(f'no vectors for word {w1}')\n",
    "            return None\n",
    "\n",
    "        v2 = self[w2]\n",
    "        if v2 is not None:\n",
    "            v2 = v2.reshape(1, -1)\n",
    "        else:\n",
    "            print(f'no vectors for word {w2}')\n",
    "            return None\n",
    "\n",
    "        return cosine_similarity(v1, v2).item()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__dict__)\n",
    "\n",
    "\n",
    "def embeddings_reader(file_name, encoding=None):\n",
    "    \"\"\"\n",
    "    generator function for reading in word vector files\n",
    "    \"\"\"\n",
    "    for line in tqdm(open(file_name, \"r\", encoding=encoding)):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        yield word, coefs\n",
    "\n",
    "\n",
    "def read_embeddings(embedding_type, base_dir='./word_vectors'):\n",
    "    \"\"\"\n",
    "    read in files word embedding files\n",
    "    input:\n",
    "        - type of word embeddings (word2vec, GloVe, ConceptNet Numberbatch)\n",
    "        - directory where files are stored (defaults to data/word_vectors in this repo)\n",
    "    output:\n",
    "        - numpy.ndarray with word vectors, list with corresponding labels\n",
    "    \"\"\"\n",
    "\n",
    "    if embedding_type.casefold() == 'word2vec':\n",
    "\n",
    "        w2w_path = join(base_dir, 'ger_word2vec_vectors.txt')\n",
    "\n",
    "        word_vectors = list()\n",
    "        vector_labels = list()\n",
    "\n",
    "        for word, coefs in embeddings_reader(w2w_path):\n",
    "            word_vectors.append(coefs)\n",
    "            vector_labels.append(word[2:-1])\n",
    "\n",
    "    elif embedding_type.casefold() == 'glove':\n",
    "\n",
    "        glove_path = join(base_dir, 'ger_glove_vectors.txt')\n",
    "\n",
    "        word_vectors = list()\n",
    "        vector_labels = list()\n",
    "\n",
    "        for word, coefs in embeddings_reader(glove_path):\n",
    "            word_vectors.append(coefs)\n",
    "            vector_labels.append(word)\n",
    "\n",
    "        # convert to np array\n",
    "        word_vectors = np.vstack(word_vectors)\n",
    "\n",
    "    elif embedding_type.casefold() in ['conceptnetnb', 'conceptnet']:\n",
    "\n",
    "        cnnb_path = join(base_dir, 'conceptnet_vectors.txt')\n",
    "\n",
    "        word_vectors = list()\n",
    "        vector_labels = list()\n",
    "\n",
    "        prefix = '/c/de/'\n",
    "\n",
    "        for word, coefs in embeddings_reader(cnnb_path):\n",
    "            if word.startswith(prefix):\n",
    "                word_vectors.append(coefs)\n",
    "                vector_labels.append(word.replace(prefix, ''))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f'invalid embedding type {embedding_type}: Choose either word2vec, GloVe or ConceptNetNB!')\n",
    "\n",
    "    return word_vectors, vector_labels\n",
    "\n",
    "\n",
    "def make_embedding_map(word_vectors, vector_labels):\n",
    "    \"\"\"\n",
    "    transform word vectors and vector labels as dict (with structure word -> vector)\n",
    "    \"\"\"\n",
    "\n",
    "    embedding_map = dict(zip(vector_labels, word_vectors))\n",
    "\n",
    "    return embedding_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_A = [\"Trommel\", \"Vorhang\", \"Glocke\", \"Kaffee\", \"Schule\", \"Eltern\", \"Mond\", \"Garten\", \"Hut\", \"Bauer\", \"Nase\", \"Truthahn\", \"Farbe\", \"Haus\", \"Fluss\"]\n",
    "List_C = [\"Geige\", \"Fenster\", \"Lampe\", \"Museum\", \"Tee\", \"Reise\", \"Sonne\", \"Wiese\", \"Treppe\", \"Maurer\", \"Zunge\", \"Tiger\", \"Musik\", \"Stadt\", \"See\"]\n",
    "List_D = [\"Horn\", \"Tür\", \"Seil\", \"Kakao\", \"Gericht\", \"Wagen\", \"Sterne\", \"Baum\", \"Mantel\", \"Pfarrer\", \"Mund\", \"Gans\", \"Form\", \"Land\", \"Regen\"]\n",
    "\n",
    "lists = [List_A, List_C, List_D]\n",
    "list_labels = ['List_A', 'List_C', 'List_D']\n",
    "named_lists = list(zip(list_labels, lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load word2vec embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "854776it [00:34, 24672.75it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 13.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute similarities for List_A with word2vec embeddings\n",
      "save to file ./out/List_A_word2vec.csv\n",
      "compute similarities for List_C with word2vec embeddings\n",
      "save to file ./out/List_C_word2vec.csv\n",
      "compute similarities for List_D with word2vec embeddings\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "no vectors for word Tür\n",
      "save to file ./out/List_D_word2vec.csv\n",
      "Load GloVe embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1309281it [00:56, 23221.41it/s]\n",
      " 33%|███▎      | 1/3 [00:00<00:00,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute similarities for List_A with GloVe embeddings\n",
      "save to file ./out/List_A_GloVe.csv\n",
      "compute similarities for List_C with GloVe embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to file ./out/List_C_GloVe.csv\n",
      "compute similarities for List_D with GloVe embeddings\n",
      "save to file ./out/List_D_GloVe.csv\n",
      "Load ConceptNet embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9161913it [06:11, 24673.20it/s]\n",
      " 67%|██████▋   | 2/3 [00:00<00:00, 13.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute similarities for List_A with ConceptNet embeddings\n",
      "save to file ./out/List_A_ConceptNet.csv\n",
      "compute similarities for List_C with ConceptNet embeddings\n",
      "save to file ./out/List_C_ConceptNet.csv\n",
      "compute similarities for List_D with ConceptNet embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 14.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to file ./out/List_D_ConceptNet.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for embedding_type in ['word2vec', 'GloVe', 'ConceptNet']:\n",
    "\n",
    "    print(f'Load {embedding_type} embeddings...')\n",
    "    embeddings = WordEmbeddings(embedding_type)\n",
    "\n",
    "    for list_name, list_words in tqdm(named_lists):\n",
    "\n",
    "        key = f'{list_name}_{embedding_type}'\n",
    "        \n",
    "        print(f'compute similarities for {list_name} with {embedding_type} embeddings')\n",
    "        \n",
    "        df = pd.DataFrame(columns=list_words, index=list_words)\n",
    "        word_combinations = list(product(list_words, repeat=2))\n",
    "\n",
    "        for w1, w2 in word_combinations:\n",
    "            df[w1][w2] = embeddings.get_cosine_similarity(w1, w2)\n",
    "        \n",
    "        fname = join('./out', f'{key}.csv')\n",
    "        print(f'save to file {fname}')\n",
    "        df.to_csv(fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "word_similarities",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
